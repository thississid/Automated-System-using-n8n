# Automated-System-using-n8n

An **open-source AI-powered customer support system** that utilizes a **local LLM** (via **Ollama**) combined with **LangChain FAISS** as an **in-memory vector store** to retrieve past support solutions. The system is built with **FastAPI** for the backend and uses **n8n** for workflow automation. The entire setup is **containerized using Docker Compose** for easy deployment and scalability.

---

## **üöÄ Features**

- ‚úÖ **Local LLM with Ollama** ‚Äì Runs models like **Mistral locally**, eliminating API costs.
- ‚úÖ **Knowledge Base** ‚Äì Uses **LangChain FAISS** to store and retrieve past support queries and solutions.
- ‚úÖ **FastAPI Backend** ‚Äì Processes customer queries, searches the knowledge base, and generates AI-driven responses.
- ‚úÖ **n8n Workflow Automation** ‚Äì Automates query handling, response generation, and email/messaging notifications.
- ‚úÖ **Dockerized Deployment** ‚Äì Uses **Docker Compose** to seamlessly manage all services.

---

## **üìÇ Project Workflow**

1. **Customer Query Handling** ‚Äì A user submits a query via **n8n Webhook**.
2. **Processing & Retrieval** ‚Äì **FastAPI backend** searches the in-memory vector database (FAISS) for similar past queries.
3. **AI-Powered Response Generation** ‚Äì **Ollama LLM** generates a response based on the query and retrieved solutions.
4. **Response Delivery** ‚Äì The generated response is sent back to the user via **email, chat, or another preferred method** using n8n.
5. **Containerized System** ‚Äì **Docker Compose** ensures that all components (Ollama, FastAPI, n8n) run efficiently and communicate seamlessly.

---

## **üõ†Ô∏è Installation & Setup**

### **1Ô∏è‚É£ Prerequisites**

Ensure you have the following installed:

- [Docker & Docker Compose](https://docs.docker.com/get-docker/)
- [Ollama](https://ollama.com/)

---

### **2Ô∏è‚É£ Clone the Repository**

```bash
git clone https://github.com/your-username/Automated-Support-AI.git
cd Automated-Support-AI
```

### **3Ô∏è‚É£ Set Up Ollama**

Install Ollama and pull a model (e.g., Mistral):

```bash
ollama pull mistral
```

### **4Ô∏è‚É£ Run the FastAPI Backend**

Navigate to the `backend/` directory and install dependencies:

```bash
cd backend
pip install -r requirements.txt
```

Run the FastAPI server:

```bash
uvicorn main:app --reload
```

### **5Ô∏è‚É£ Set Up n8n Workflow**

- Open n8n UI at [http://localhost:5678](http://localhost:5678/).
- Import the workflow file: workflows/support-workflow.json.
- Configure webhook and response handling in n8n.

### **6Ô∏è‚É£ Access the Services**

- FastAPI Backend: [http://localhost:8000](http://localhost:8000)
- n8n UI: [http://localhost:5678](http://localhost:5678)

## **‚úÖ Summary of Additions:**  
‚úî **Included all necessary commands**  
‚úî **Step-by-step installation guide**  
‚úî **How to start the project manually & using Docker**  
‚úî **How to set up n8n & Ollama**  